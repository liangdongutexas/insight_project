{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "493a8231a448b2727bc7d2eb2f995bf6a6b0c5eb"
   },
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "a1b737b2491d22b501f4d53c70ab811ce1ce8471"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, I will make the previous web scraping codes into a pipelin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. functionget_hyperlinks will created a csv file with the name: component_name_hyperlinks.csv. This file stores all the hyperlinks of each item in the search page of url from newegg website.\n",
    "   a. component_name is the name of the computer components i.e intel_cpu\n",
    "   b. The url should be in the following form url = \"https://www.newegg.com/p/pl?N=100007671%20601306860&Page={}\"\n",
    "   c. num_pages is the total number of pages to be web scraped in the searching page of the url at newegg website\n",
    "   \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping all Hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperlinks(component_name,url,num_pages):\n",
    "    ## create a csv table to store all the hyperlinks of CPU\n",
    "    filename = \"{}_hyperlinks.csv\".format(component_name)\n",
    "    f = open(filename, \"w\", encoding='utf-8')\n",
    "    headers = \"product_detail,hyperlink\\n\"\n",
    "    f.write(headers)\n",
    "    \n",
    "    \n",
    "    # Scraping num_pages of my_url from newegg\n",
    "\n",
    "    for i in range(1, num_pages+1):\n",
    "        my_url = url.format(i)\n",
    "        uClient = request.urlopen(my_url)\n",
    "        page_html = uClient.read()\n",
    "        uClient.close()\n",
    "        page_soup = BeautifulSoup(page_html, \"html.parser\")\n",
    "        containers = page_soup.findAll(\"div\", {\"class\": \"item-container\"})\n",
    "\n",
    "        for container in containers:\n",
    "            ## Find hyperlink that directs to the webpage of that particular product\n",
    "            hyperlink = list(container.children)[3]['href']\n",
    "\n",
    "            product_detail = list(container.children)[3].img[\"alt\"].replace(\",\", \"  \")\n",
    "\n",
    "\n",
    "            f.write(product_detail+\",\"+hyperlink+\"\\n\")\n",
    "\n",
    "            time.sleep(random.random()+0.5)\n",
    "        time.sleep(10+random.random())\n",
    "        print(i)\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all specs from the hyperlink_file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## return a list of specs by webscraping all the hyperlinks in the \n",
    "#  component_name+\"_hyperlinks.csv\" file created by get_hyperlinks function \n",
    "def get_specs(component_name):\n",
    "    hyperlinks=pd.read_csv(component_name+\"_hyperlinks.csv\")\n",
    "    \n",
    "    ## all cpu specs will be stored in the list total_specs\n",
    "    ## then total_specs will be stored in a csv table\n",
    "    total_specs=[]\n",
    "    i=0\n",
    "    \n",
    "    for url in hyperlinks['hyperlink']:\n",
    "        uClient = request.urlopen(url)\n",
    "        page_html = uClient.read()\n",
    "        uClient.close()\n",
    "        page_soup = BeautifulSoup(page_html, \"lxml\")  \n",
    "\n",
    "        Specs={}\n",
    "        specs = page_soup.find(\"div\", {\"id\": \"Specs\"})\n",
    "        fieldsets=specs.findAll(\"fieldset\")\n",
    "        for fieldset in fieldsets:\n",
    "            dls=fieldset.findAll(\"dl\")\n",
    "            for dl in dls:\n",
    "                Specs[dl.find(\"dt\").text]=dl.find(\"dd\").text\n",
    "\n",
    "        total_specs.append(Specs)\n",
    "        i+=1\n",
    "        print(i)\n",
    "\n",
    "        time.sleep(10+random.random())\n",
    "        \n",
    "    return total_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write all the specs into a csv file called component_name+\"_specs.csv\"\n",
    "def write_specs(total_specs,component_name):\n",
    "    ## find the maximal number of header titles for the csv file\n",
    "    headers=total_specs[0]\n",
    "    for i in total_specs[1:]:\n",
    "        for j in i.keys():\n",
    "            if j not in headers:\n",
    "                headers[j]=None    \n",
    "    \n",
    "    ## make a string file for the header\n",
    "    headerstr=''\n",
    "    for i in list(headers.keys()):\n",
    "        headerstr=headerstr+i+','\n",
    "    headerstr=headerstr+'\\n'      \n",
    "    \n",
    "    ## We create a csv table to store all the hyperlinks of CPU\n",
    "    filename = component_name+\"_specs.csv\"\n",
    "    f = open(filename, \"w\", encoding='utf-8')\n",
    "    f.write(headerstr)\n",
    "    \n",
    "    ## write specs information into the csv file\n",
    "    headers_list=list(headers.keys())\n",
    "    for item in total_specs:\n",
    "        item_str=''\n",
    "        for col in headers_list:\n",
    "            item_str=item_str+str(item.get(col,'None')).replace(',',' ')+','\n",
    "        item_str=item_str[:-1]\n",
    "        item_str=item_str+'\\n'\n",
    "        f.write(item_str) \n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is the total pipeline\n",
    "def pipeline(component_name,url,num_pages):\n",
    "    get_hyperlinks(component_name,url,num_pages)\n",
    "    total_specs=get_specs(component_name)\n",
    "    write_specs(total_specs,component_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "url='https://www.newegg.com/p/pl?N=100007671%20601306869&Page={}'\n",
    "component_name='amd_cpu'\n",
    "num_pages=4\n",
    "pipeline(component_name,url,num_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
